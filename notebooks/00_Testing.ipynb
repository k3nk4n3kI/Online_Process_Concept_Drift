{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Drift and Online Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sources:\n",
    "    - https://medium.com/analytics-vidhya/tf-gradienttape-explained-for-keras-users-cc3f06276f22\n",
    "    - https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n",
    "    - https://www.kaggle.com/code/fabriciojoc/drebin-experiment-4-adwin-retrain\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kein dynamic padding benötigt, da jede Sequenz einzeln preprocessed und für prediction verwendet wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Append the directory containing the src folder to sys.path\n",
    "sys.path.append('/Users/lars/Documents/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path variables for datasets\n",
    "directory = \"/Users/lars/Documents/Uni/Masterarbeit/Online_Process_Concept_Drift\"\n",
    "path_raw = \"/data/raw/\"\n",
    "path_interim = \"/data/interim/\"\n",
    "path_processed = \"/data/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lars/Documents/test/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_models as tfm\n",
    "from river import drift\n",
    "from transformers import TFAutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from src.data.data_manager import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.\n",
      "2024-07-12 12:26:33.876010: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-07-12 12:26:33.876028: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-07-12 12:26:33.876031: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-07-12 12:26:33.876062: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-07-12 12:26:33.876078: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "\n",
    "max_length = 36\n",
    "batch_size = 1\n",
    "num_classes = 10\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=5e-5)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = data_loader(directory, path_interim, \"Long_Helpdesk_train\")\n",
    "val_tensor = data_loader(directory, path_interim, \"Long_Helpdesk_val\")\n",
    "test_tensor = data_loader(directory, path_interim, \"Long_Helpdesk_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: idx for idx, label in enumerate(train_tensor['Next_Activity'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = train_tensor[:10]\n",
    "test_tensor = test_tensor[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Needed Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lars/Documents/test/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocessing function for samples\n",
    "def preprocessing_single(X_test, y_test):\n",
    "    X_test_encoded = tokenizer(X_test, return_tensors='tf', padding=True, truncation=True)\n",
    "    input_ids = X_test_encoded['input_ids']\n",
    "    attention_mask = X_test_encoded['attention_mask']\n",
    "    label = tf.convert_to_tensor([y_test], dtype= tf.float32)\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(tokenizer, example, max_length=512):    \n",
    "    return tokenizer(example['Prefix_Trace'], padding=False, truncation=True, max_length=max_length)\n",
    "\n",
    "def sort_by_length(dataset, tokenizer, max_length=1024):\n",
    "\n",
    "    # Tokenizes the dataset and calculates the length for all in input_ids\n",
    "    tokenized = [preprocess_function(tokenizer, example, max_length) for example in dataset]\n",
    "    lengths = [len(tok['input_ids']) for tok in tokenized]\n",
    "\n",
    "    # Combine tokenized inputs, lengths, and labels and sort them\n",
    "    combined = list(zip(tokenized, lengths, dataset['Next_Activity']))\n",
    "    combined.sort(key=lambda x: x[1])\n",
    "\n",
    "    return combined\n",
    "\n",
    "def create_buckets_and_batches_bert(sorted_data, batch_size, data_collator):\n",
    " \n",
    "    def gen():\n",
    "        while True:\n",
    "\n",
    "            # Shuffle data at the start of each epoch\n",
    "            random.shuffle(sorted_data)  \n",
    "\n",
    "            # Iterate over the dataset and select batch\n",
    "            for i in range(0, len(sorted_data), batch_size):\n",
    "                batch = sorted_data[i:i + batch_size]\n",
    "                \n",
    "                # Skip empty batches\n",
    "                if len(batch) == 0:\n",
    "                    continue  \n",
    "                \n",
    "                # Extract tokenized inputs and labels from the batch\n",
    "                tokenized_batch = [item[0] for item in batch]\n",
    "                labels = [item[2] for item in batch]\n",
    "                \n",
    "                # Create input dictionaries\n",
    "                batch_inputs = {'input_ids': [tok['input_ids'] for tok in tokenized_batch],\n",
    "                                'attention_mask': [tok['attention_mask'] for tok in tokenized_batch]}\n",
    "                \n",
    "                # Batch the inputs and yiel the batches and labels as tensors\n",
    "                batch_inputs = data_collator(batch_inputs)\n",
    "                yield dict(batch_inputs), tf.convert_to_tensor(labels)\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            {'input_ids': tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "             'attention_mask': tf.TensorSpec(shape=(None, None), dtype=tf.int32)},\n",
    "            tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTOnline:\n",
    "    def __init__(self, model_name, num_classes):\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        # Load the pretrained BERT model\n",
    "        encoder = TFAutoModel.from_pretrained(self.model_name)\n",
    "\n",
    "        # Input layer for input_ids and attention_masks\n",
    "        input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_ids')\n",
    "        attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "        # Get encoder outputs\n",
    "        encoder_outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Get the pooled output and make sure it is of type tf.float32\n",
    "        pooled_output = tf.keras.layers.Lambda(lambda x: tf.cast(x.pooler_output, tf.float32))(encoder_outputs)\n",
    "\n",
    "        # Apply dropout\n",
    "        dropout = tf.keras.layers.Dropout(rate=0.1)(pooled_output)\n",
    "\n",
    "        # Final dense layer for classification with softmax activation function and L2 regularization\n",
    "        output = tf.keras.layers.Dense(self.num_classes, activation='softmax', dtype=tf.float32)(dropout)\n",
    "        \n",
    "        # Create model\n",
    "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lars/Documents/test/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "test = BERTOnline(model_name='bert-base-uncased', num_classes=10)\n",
    "test = test.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x3b1a8d990>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_load_path = '/Users/lars/Documents/test/models/Weights_Helpdesk_Tuned/Weights_Helpdesk_Tuned'\n",
    "test.load_weights(weights_load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocessing function for samples\n",
    "def preprocessing_single(X_test, y_test):\n",
    "    X_test_encoded = tokenizer(X_test, return_tensors='tf', padding=True, truncation=True)\n",
    "    input_ids = X_test_encoded['input_ids']\n",
    "    attention_mask = X_test_encoded['attention_mask']\n",
    "    label = tf.convert_to_tensor([y_test], dtype=tf.int32)  # Change to int32 for SparseCategoricalCrossentropy\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, loss_fn, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict\n",
    "        predictions = model(x, training=True)\n",
    "        # Calculate Loss\n",
    "        loss = loss_fn(y, predictions)\n",
    "    \n",
    "    # Calculate Gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Update model\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding labels and preparing samples to be tokenized later on\n",
    "#label_map = {label: idx for idx, label in enumerate(train_tensor['Next_Activity'].unique())}\n",
    "y_test = test_tensor['Next_Activity'].map(label_map).astype(int).to_numpy()\n",
    "y_train = train_tensor['Next_Activity'].map(label_map).astype(int).to_numpy()\n",
    "X_test = test_tensor['Prefix_Trace'].astype(str).values.tolist()\n",
    "X_train = train_tensor['Next_Activity'].astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for tracking\n",
    "drifts = []\n",
    "warnings = []\n",
    "DRIFT = False\n",
    "WARNING = False\n",
    "warning_data = []\n",
    "y_warning = []\n",
    "acc = []\n",
    "pred = []\n",
    "true = []\n",
    "hits = 0\n",
    "p = []\n",
    "s = []\n",
    "n = 1.0\n",
    "p.append(1.0)\n",
    "\n",
    "X_window = X_train\n",
    "y_window = y_train\n",
    "\n",
    "adwin = drift.ADWIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Sample 0, Loss: 0.23538294434547424\n",
      "Start\n",
      "Sample 1, Loss: 0.18390701711177826\n",
      "Start\n",
      "Sample 2, Loss: 1.0428751707077026\n",
      "Start\n",
      "Sample 3, Loss: 1.0469597578048706\n",
      "Start\n",
      "Sample 4, Loss: 3.329148530960083\n",
      "Start\n",
      "Sample 5, Loss: 0.043817877769470215\n",
      "Start\n",
      "Sample 6, Loss: 0.8007882833480835\n",
      "Start\n",
      "Sample 7, Loss: 1.0832617282867432\n",
      "Start\n",
      "Sample 8, Loss: 3.8526039123535156\n",
      "Start\n",
      "Sample 9, Loss: 0.17193879187107086\n"
     ]
    }
   ],
   "source": [
    "# Process the training data stream\n",
    "for sample in range(len(X_train)):\n",
    "    sample_X = X_train[sample]\n",
    "    sample_y = y_train[sample]\n",
    "    \n",
    "    sample_X, label = preprocessing_single(sample_X, sample_y)\n",
    "    print(\"Start\")\n",
    "    # Train on the sample\n",
    "    loss = train_step(test, optimizer, loss_fn, sample_X, label)\n",
    "    print(f\"Sample {sample}, Loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model for evaluation\n",
    "test.compile(optimizer=optimizer, loss=loss_fn, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 10\n",
      "Steps per epoch (train): 10\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "window_data = {\n",
    "    'Prefix_Trace': X_test,\n",
    "    'Next_Activity': y_test.tolist()\n",
    "}\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "window_data = Dataset.from_dict(window_data)\n",
    "\n",
    "# Sort the data by length\n",
    "sorted_window_data = sort_by_length(window_data, tokenizer, max_length)\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "# Create TensorFlow datasets and ensure they repeat\n",
    "tf_window_dataset = create_buckets_and_batches_bert(sorted_window_data, batch_size, data_collator).repeat()\n",
    "\n",
    "# Prefetch datasets\n",
    "tf_window_dataset = tf_window_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Calculate steps per epoch based on the length of the dataset\n",
    "window_steps_per_epoch = len(sorted_window_data) // batch_size\n",
    "\n",
    "# Debugging statements to check the sizes and steps\n",
    "print(f\"Number of training samples: {len(sorted_window_data)}\")\n",
    "print(f\"Steps per epoch (train): {window_steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 12:27:41.579837: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 12s 778ms/step - loss: 0.7615 - acc: 0.7000\n",
      "Validation loss: 0.7614604234695435\n",
      "Validation accuracy: 0.699999988079071\n"
     ]
    }
   ],
   "source": [
    "# Test if everything worked\n",
    "evaluation = test.evaluate(tf_window_dataset, steps=window_steps_per_epoch)\n",
    "\n",
    "print(f\"Validation loss: {evaluation[0]}\")\n",
    "print(f\"Validation accuracy: {evaluation[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start formating data for streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding labels and preparing samples to be tokenized later on\n",
    "#label_map = {label: idx for idx, label in enumerate(train_tensor['Next_Activity'].unique())}\n",
    "y_test = test_tensor['Next_Activity'].map(label_map).astype(int).to_numpy()\n",
    "y_train = train_tensor['Next_Activity'].map(label_map).astype(int).to_numpy()\n",
    "X_test = test_tensor['Prefix_Trace'].astype(str).values.tolist()\n",
    "X_train = train_tensor['Next_Activity'].astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up lists for information storage\n",
    "# drift points\n",
    "drifts = []\n",
    "\n",
    "# warning points\n",
    "warnings = []\n",
    "\n",
    "# flags for drift and warning\n",
    "DRIFT = False\n",
    "WARNING = False\n",
    "warning_data = []\n",
    "y_warning = []\n",
    "\n",
    "# accuracy\n",
    "acc = []\n",
    "\n",
    "# predictions\n",
    "pred = []\n",
    "true = []\n",
    "\n",
    "# hits\n",
    "hits = 0\n",
    "\n",
    "# prequential error\n",
    "p = [] \n",
    "\n",
    "# stdev\n",
    "s = [] \n",
    "\n",
    "# counte\n",
    "n = 1.0 \n",
    "\n",
    "p.append(1.0)\n",
    "\n",
    "X_window = X_train\n",
    "y_window = y_train\n",
    "\n",
    "adwin = drift.ADWIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModelBuilderDynamic:\n",
    "    def __init__(self, model_name, num_classes):\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        # Load the pretrained BERT model\n",
    "        encoder = TFAutoModel.from_pretrained(self.model_name)\n",
    "\n",
    "        # Input layer for input_ids and attention_masks\n",
    "        input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_ids')\n",
    "        attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "        # Get encoder outputs\n",
    "        encoder_outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Get the pooled output and make sure it is of type tf.float32\n",
    "        pooled_output = tf.keras.layers.Lambda(lambda x: tf.cast(x.pooler_output, tf.float32))(encoder_outputs)\n",
    "\n",
    "        # Apply dropout\n",
    "        dropout = tf.keras.layers.Dropout(rate=0.1)(pooled_output)\n",
    "\n",
    "        # Final dense layer for classification with softmax activation function and L2 regularization\n",
    "        output = tf.keras.layers.Dense(self.num_classes, activation='softmax', dtype=tf.float32)(dropout)\n",
    "        \n",
    "        # Create model\n",
    "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "        # Compile model with AdamW as optimzer\n",
    "        model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=5e-5),\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# BERT for online learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTOnline:\n",
    "    def __init__(self, model_name, num_classes):\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        # Load the pretrained BERT model\n",
    "        encoder = TFAutoModel.from_pretrained(self.model_name)\n",
    "\n",
    "        # Input layer for input_ids and attention_masks\n",
    "        input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_ids')\n",
    "        attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "        # Get encoder outputs\n",
    "        encoder_outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Get the pooled output and make sure it is of type tf.float32\n",
    "        pooled_output = tf.keras.layers.Lambda(lambda x: tf.cast(x.pooler_output, tf.float32))(encoder_outputs)\n",
    "\n",
    "        # Apply dropout\n",
    "        dropout = tf.keras.layers.Dropout(rate=0.1)(pooled_output)\n",
    "\n",
    "        # Final dense layer for classification with softmax activation function and L2 regularization\n",
    "        output = tf.keras.layers.Dense(self.num_classes, activation='softmax', dtype=tf.float32)(dropout)\n",
    "        \n",
    "        # Create model\n",
    "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lars/Documents/test/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "test = BERTOnline(model_name='bert-base-uncased', num_classes=10)\n",
    "test = test.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x3904e9810>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_load_path = '/Users/lars/Documents/test/models/Weights_Helpdesk_Tuned/Weights_Helpdesk_Tuned'\n",
    "test.load_weights(weights_load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online(sample, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict\n",
    "        pred_y = test(sample)\n",
    "        # Calculate Loss\n",
    "        model_loss = loss(label, pred_y)\n",
    "    \n",
    "    # Calculate Gradients\n",
    "    model_gradients = tape.gradient(model_loss, test.trainable_variables)\n",
    "\n",
    "    # Update model\n",
    "    optimizer.apply_gradients(zip(model_gradients, test.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 12:09:54.827992: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m sample_X, label \u001b[38;5;241m=\u001b[39m preprocessing_single(sample_X, sample_y)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(label)\n\u001b[0;32m----> 7\u001b[0m \u001b[43monline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/test/venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/test/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Documents/test/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:905\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    902\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    909\u001b[0m   bound_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m    910\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds\n\u001b[1;32m    911\u001b[0m   )\n",
      "File \u001b[0;32m~/Documents/test/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/test/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Documents/test/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Documents/test/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/test/venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/Documents/test/venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sample in range(len(X_train)):\n",
    "    sample_X = X_train[sample]\n",
    "    sample_y = y_train[sample]\n",
    "    print(sample_y)\n",
    "    sample_X, label = preprocessing_single(sample_X, sample_y)\n",
    "    print(label)\n",
    "    online(sample_X, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['acc']) # Compile just for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 10\n",
      "Steps per epoch (train): 10\n"
     ]
    }
   ],
   "source": [
    "window_data = {\n",
    "    'Prefix_Trace' : X_test,\n",
    "    'Next_Activity': y_test.tolist()\n",
    "}\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "window_data = Dataset.from_dict(window_data)\n",
    "\n",
    "# Sort the data by length\n",
    "sorted_window_data = sort_by_length(window_data, tokenizer, max_length)\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "# Create TensorFlow datasets and ensure they repeat\n",
    "tf_window_dataset = create_buckets_and_batches_bert(sorted_window_data, batch_size, data_collator).repeat()\n",
    "\n",
    "# Prefetch datasets\n",
    "tf_window_dataset = tf_window_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Calculate steps per epoch based on the length of the dataset\n",
    "window_steps_per_epoch = len(sorted_window_data) // batch_size\n",
    "\n",
    "# Debugging statements to check the sizes and steps\n",
    "print(f\"Number of training samples: {len(sorted_window_data)}\")\n",
    "print(f\"Steps per epoch (train): {window_steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 12:07:12.073091: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Creating variables on a non-first call to a function decorated with tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test if everything worked\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_window_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_steps_per_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluation[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluation[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/test/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/test/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:881\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m   results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    878\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    879\u001b[0m   )\n\u001b[1;32m    880\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m--> 881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    882\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    883\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Creating variables on a non-first call to a function decorated with tf.function."
     ]
    }
   ],
   "source": [
    "# Test if everything worked\n",
    "evaluation = test.evaluate(tf_window_dataset, steps=window_steps_per_epoch)\n",
    "\n",
    "print(f\"Validation loss: {evaluation[0]}\")\n",
    "print(f\"Validation accuracy: {evaluation[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the model\n",
    "model = BERTModelBuilderDynamic(model_name='bert-base-uncased', num_classes=10)\n",
    "model = model.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x3ab67a020>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the weights into model\n",
    "\n",
    "weights_load_path = '/Users/lars/Documents/test/models/Weights_Helpdesk_Tuned/Weights_Helpdesk_Tuned'\n",
    "model.load_weights(weights_load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in range(len(X_test)):\n",
    "    sample_X = X_test[sample]\n",
    "    sample_y = y_test[sample]\n",
    "\n",
    "    # add unprocessed samples to window\n",
    "    X_window = np.append(X_window, sample_X)\n",
    "    y_window = np.append(y_window, sample_y)\n",
    "\n",
    "    # preprocess data to be used as input in BERT\n",
    "    sample_X, label = preprocessing_single(sample_X, sample_y)\n",
    "    y_pred = model.predict(sample_X)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    print(y_pred)\n",
    "\n",
    "    # Save prediction and true value\n",
    "    pred.append(y_pred)\n",
    "    true.append(sample_y)\n",
    "\n",
    "    # check if prediction is a hit\n",
    "    if y_pred == sample_y:\n",
    "        hits += 1\n",
    "        p.append(p[-1]-p[-1]/n)\n",
    "    else:\n",
    "        p.append(p[-1]+(1-p[-1])/n)\n",
    "    \n",
    "    # calculate stdv\n",
    "    s.append(math.sqrt(p[-1]*(1-p[-1])/n))\n",
    "    n += 1\n",
    "\n",
    "    # update drift detector\n",
    "    adwin.update(int(sample_y ==  y_pred))\n",
    "\n",
    "    # save accuracy\n",
    "    acc.append(float(hits)/float(sample+1))\n",
    "\n",
    "    if drift.drift_detected:\n",
    "        print(f\"Change has been detected in {sample}\")\n",
    "        print(f\"Window size: {adwin.width}\")\n",
    "        print(f\"Total sum of stored elements: {adwin.total}\")\n",
    "        print(f\"Mean: {adwin.estimation}\")\n",
    "        print(f\"Variance: {adwin.variance}\")\n",
    "        print(f\"Total number of drifts: {adwin.n_detections}\")\n",
    "\n",
    "        p.append(1.0)\n",
    "        s.append(0.0)\n",
    "        n = 1.0\n",
    "        \n",
    "        drifts.append(sample)\n",
    "\n",
    "        # update data and label window\n",
    "        X_window = X_window[-len(X_window) - adwin.width:]\n",
    "        y_window = y_window[-len(y_window) - adwin.width:]\n",
    "\n",
    "        # preprocess X_window with dynamic padding\n",
    "        # Create dictionary for tf dataset creation\n",
    "        window_data = {\n",
    "            'Prefix_Trace' : X_window,\n",
    "            'Next_Activity': y_window.tolist()\n",
    "        }\n",
    "\n",
    "        # Convert to Hugging Face datasets\n",
    "        window_data = Dataset.from_dict(window_data)\n",
    "\n",
    "        # Sort the data by length\n",
    "        sorted_window_data = sort_by_length(window_data, tokenizer, max_length)\n",
    "\n",
    "        # Initialize data collator\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "        # Create TensorFlow datasets and ensure they repeat\n",
    "        tf_window_dataset = create_buckets_and_batches_bert(sorted_window_data, batch_size, data_collator).repeat()\n",
    "\n",
    "        # Prefetch datasets\n",
    "        tf_window_dataset = tf_window_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        # Calculate steps per epoch based on the length of the dataset\n",
    "        window_steps_per_epoch = len(sorted_window_data) // batch_size\n",
    "\n",
    "        # Debugging statements to check the sizes and steps\n",
    "        print(f\"Number of training samples: {len(sorted_window_data)}\")\n",
    "        print(f\"Steps per epoch (train): {window_steps_per_epoch}\")\n",
    "\n",
    "        # BERT with GradientTape\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        adwin = drift.ADWIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on a test dataset (optional)\n",
    "predictions = test.predict(tf_test_dataset, steps=test_steps_per_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
